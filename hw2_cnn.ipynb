{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the CIFAR-10\n",
    "def load_CIFAR10(pos, n_chunks=1):\n",
    "    Xtr = []\n",
    "    Ytr = []\n",
    "    for i in range(n_chunks):\n",
    "        train = unpickle(pos + '/data_batch_{0}'.format(i + 1))\n",
    "        Xtr.extend(train[b'data'])\n",
    "        Ytr.extend(train[b'labels'])\n",
    "    test = unpickle(pos + '/test_batch')\n",
    "    Xte = test[b'data']\n",
    "    Yte = test[b'labels']\n",
    "    return np.array(Xtr), np.array(Ytr), np.array(Xte), np.array(Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# expresses the label data in one-hot encoding.\n",
    "def onehot_encoding (Ytr, Yte):\n",
    "    Ytr_onehot = np.eye(10)[Ytr]\n",
    "    Yte_onehot = np.eye(10)[Yte]\n",
    "    return Ytr_onehot, Yte_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the train and test data\n",
    "Xtr, Ytr, Xte, Yte = load_CIFAR10('cifar-10-batches-py', 5)\n",
    "                                 \n",
    "# image data, each data size is 32*32*3\n",
    "Xtr = Xtr.reshape(50000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "Xte= Xte.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "\n",
    "# label data of train and test data, label data is represented by one-hot encoding\n",
    "Ytr, Yte = onehot_encoding(Ytr, Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, sess, beta, gamma, n_row):\n",
    "        self.sess = sess\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = None\n",
    "        self.n_row = n_row\n",
    "    def modeling(self, X):\n",
    "        L1 = self.Layer(X, 7, 3, 64, 2, 'Layer_1') # filter 7x7x3x64, stride 2\n",
    "        L1_pooling = self.pooling(L1, 3, 2)\n",
    "        L2_1 = self.Layer(L1_pooling, 1, 64, 64, 1, 'Layer_2_1' pad='VALID')\n",
    "        L2_2 = self.Layer(L2_1, 3, 64, 64, 1, 'Layer_2_2', pad='VALID')\n",
    "        L2_3 = self.Layer(L2_2, 1, 64, 256, 1, 'Layer_2_3', pad='VALID')\n",
    "        L2_4 = self.Layer(L1_pooling, 1, 64, 256, 1, 'Layer_2_4', pad='VALID')\n",
    "        L2 = tf.nn.relu(L2_3 + L2_4)\n",
    "        Layer = self.pooling(L2, 56, 1)\n",
    "        return Layer\n",
    "    def _batch(self, iterable, length, n=1):\n",
    "        for ndx in range(0, length, n):\n",
    "            yield iterable[ndx:min(ndx + n, length)]\n",
    "    def train(self, X, y, n_epoch, batch_size):\n",
    "        n_row = self.n_row\n",
    "        sess = self.sess\n",
    "        self.batch_size = batch_size\n",
    "        n_step = n_row // batch_size\n",
    "        loss = self.loss_funtion(X, y)\n",
    "        for epoch in range(n_epoch):\n",
    "            arr = np.random.randint(n_row, size=n_row)\n",
    "            for x in self._batch(range(n_row), n_row, batch_size):\n",
    "                batch_X, batch_y = X[arr[x], :], y[arr[x]]\n",
    "                loss_ = sess.run(loss, feed_fict:{X:batch_X, y:batch_y})\n",
    "                print(loss_)\n",
    "#             for step in range(n_step):\n",
    "#                 pass\n",
    "    def loss_funtion(self, X, y, learning_rate=0.001):\n",
    "        Layer = self.modeling(X)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Layer, labels=y))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "        return loss\n",
    "    def Layer(self, X, filter_size, in_feature, out_feature, stride, name, pad = 'SAME'):\n",
    "        with tf.variable_scope(name):\n",
    "            L = self.conv_bn_relu(X, filter_size, in_feature, out_feature, stride, pad = 'SAME')\n",
    "        return L\n",
    "    def conv_bn_relu(self, X, filter_size, in_feature, out_feature, stride, pad = 'SAME'):\n",
    "        batch, beta, gamma = self.batch_size, self.beta, self.gamma\n",
    "        stddev = math.sqrt(2/batch) if batch else 0.01\n",
    "        W = tf.Variable(tf.random_normal([filter_size, filter_size, in_feature, out_feature], stddev=stddev))\n",
    "        L = tf.nn.conv2d(X, W, strides=[1,stride,stride,1], padding=pad)\n",
    "#         L = tf.layers.batch_normalization(L, training=True)\n",
    "        L = tf.nn.batch_normalization(L, offset=beta, scale=gamma)\n",
    "        L = tf.nn.relu(L)\n",
    "        return L\n",
    "    def pooling(self, L, ksize, stride):\n",
    "        L = tf.nn.max_pool(L, ksize=[1, ksize, ksize, 1], strides=[1, stride, stride, 1], padding='SAME')\n",
    "        return L\n",
    "    def save_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Implement the layers of CNNs ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost function, you can change the implementation\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Implement the train process ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Implement the test process ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
